{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as tr\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import TrajectoryDataset\n",
    "from lightly.models.modules.heads import VICRegProjectionHead\n",
    "from encoder_train import save_model, compute_mean_and_std, get_byol_transforms, get_encoder_loss\n",
    "from encoder_train import criterion as VICReg_criterion\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting the Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data_points 62\n",
      "Shape of state: torch.Size([16, 17, 2, 65, 65])\n",
      "Shape of action: torch.Size([16, 16, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gokul/miniconda3/envs/ml/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:285: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n"
     ]
    }
   ],
   "source": [
    "dataset = TrajectoryDataset(\n",
    "    data_dir = \"../dataset/\",\n",
    "    states_filename = \"states.npy\",\n",
    "    actions_filename = \"actions.npy\",\n",
    "    s_transform = None,\n",
    "    a_transform = None,\n",
    "    length = 992    \n",
    ")\n",
    "\n",
    "# TODO: create two dataset for train and test\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "first_datapoint = next(iter(dataloader))\n",
    "state, action = first_datapoint\n",
    "\n",
    "print(f\"Number of data_points {len(dataloader)}\")\n",
    "print(f\"Shape of state: {state.shape}\")\n",
    "print(f\"Shape of action: {action.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining the Model\n",
    "\n",
    "1. `Encoder`: which will be a simple CNN network.\n",
    "2. `Predictor`: which will be a simple LSTM Cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, input_channel=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channel, 12, padding=1, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(12, 12, padding=1, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(12, 12, padding=1, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(12)\n",
    "        self.bn2 = nn.BatchNorm2d(12)\n",
    "        self.bn3 = nn.BatchNorm2d(12)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d((5, 5), stride=2)\n",
    "        self.pool2 = nn.MaxPool2d((5, 5), stride=5)\n",
    "        # h -> (5, 5, stride=1) -> (3, 3)\n",
    "        # h = 65 -> 8748\n",
    "        self.fc1 = nn.Linear(432, 4096)\n",
    "        self.fc2 = nn.Linear(4096, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h,w = 65\n",
    "        x = self.conv1(x)        \n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x1 = x\n",
    "\n",
    "        x2 = self.conv2(x1)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        x2 = x2 + x1\n",
    "        x2 = self.pool1(x2)\n",
    "        # h,w = 31 \n",
    "\n",
    "        x3 = self.conv3(x2)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = self.relu(x3)\n",
    "        x3 = x3 + x2\n",
    "        x3 = self.pool2(x3)\n",
    "        # h,w = 6\n",
    "\n",
    "        x3 = x3.view(x3.size(0), -1)\n",
    "        # (b,12*6*6)\n",
    "        x3 = self.fc1(x3)\n",
    "        x3 = self.relu(x3)\n",
    "        x3 = self.fc2(x3)\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VICRegModel(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = VICRegProjectionHead(\n",
    "            input_dim=1024,\n",
    "            hidden_dim=1024,\n",
    "            output_dim=1024,\n",
    "            num_layers=3,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "\n",
    "        self.h = None\n",
    "        self.c = None\n",
    "\n",
    "    def set_hc(self, h, c):\n",
    "        self.h = h\n",
    "        self.c = c \n",
    "    \n",
    "    def reset_hc(self):\n",
    "        self.h = self.h.zero_() \n",
    "        self.c = self.c.zero_()\n",
    "\n",
    "    def forward(self, action):\n",
    "        self.h, self.c = self.lstm_cell(action, (self.h, self.c))\n",
    "        return self.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training \n",
    "We define `train_separate()` function, which does the training step when encoder is trained separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder(dataloader, model, optimizer, criterion, epochs, device, transformation1, \n",
    "                  transformation2, step = 1):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "            state, _ = batch\n",
    "            state = state.to(device)\n",
    "            for i in range(state.size(1)):\n",
    "                img = state[:, i, :, :, :]\n",
    "                # img = torch.cat([img, img[:, 1:2, :, :]], dim=1)\n",
    "\n",
    "                x0 = transformation1(img)\n",
    "                x1 = transformation2(img)\n",
    "\n",
    "                z0 = model(x0)\n",
    "                z1 = model(x1)\n",
    "\n",
    "                loss = criterion(z0, z1)\n",
    "                total_loss += loss.detach()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                avg_loss = total_loss / (len(dataloader)*state.size(1))\n",
    "\n",
    "        # Save model checkpoint\n",
    "        if epoch % step == 0:\n",
    "            save_model(model, epoch)\n",
    "        print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
    "    print(\"Training completed.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predictor(pred, enc, dataloader, criterion, optimizer, device, epochs=10):\n",
    "    # keeping encoder in eval mode\n",
    "    pred, enc = pred.to(device), enc.to(device)\n",
    "    enc.eval()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=\"Processing batch\"):\n",
    "            ## shape of [ s = (b, L+1, c, h, w)]  [a = (b, L, 2)]\n",
    "            s, a = batch\n",
    "            s, a = s.to(device), a.to(device)\n",
    "\n",
    "            ## initial observation\n",
    "            o = s[:, 0, :, :, :]\n",
    "            # o = torch.cat([o, o[:, 1:2, :, :]], dim=1)\n",
    "            so = enc(o)\n",
    "            co = torch.zeros(so.shape).to(device)\n",
    "            pred.set_hc(so, co)\n",
    "            \n",
    "            loss ,L = 0, a.shape[1]\n",
    "            for i in range(L):\n",
    "                sy_hat = pred(a[:, i, :])\n",
    "                temp = s[:, i+1, :, :, :]\n",
    "                # temp = torch.cat([temp, temp[:, 1:2, :, :]], dim=1)\n",
    "                sy = enc(temp)\n",
    "                loss += criterion(sy_hat, sy)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ## clearing the hidden state and cell state\n",
    "            pred.reset_hc()\n",
    "            total_loss += loss\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        save_model(pred, epoch, file_name=\"pred\")\n",
    "        print(f\"epoch: {epoch:>02}, loss: {avg_loss:.9f}\")\n",
    "    print(\"Training completed..\")\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1024\n",
    "\n",
    "encoder = SimpleEncoder(hidden_size, 2) \n",
    "encoder = VICRegModel(encoder)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = optim.SGD(encoder.parameters(), lr=0.01, momentum=0.9, weight_decay=1.5e-4)\n",
    "\n",
    "# defining transformations\n",
    "mean, std = compute_mean_and_std(dataloader, is_channelsize3=False)\n",
    "transformation1, transformation2 = get_byol_transforms(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = train_encoder(dataloader, encoder, optimizer, VICReg_criterion, 10, \n",
    "#               device, transformation1, transformation2)\n",
    "\n",
    "# save_model(encoder, \"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20854/830825227.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(\"./checkpoints/encoder__9.pth\"))\n"
     ]
    }
   ],
   "source": [
    "input_size = 2\n",
    "hidden_size = 1024\n",
    "encoder.load_state_dict(torch.load(\"./checkpoints/encoder__9.pth\"))\n",
    "\n",
    "predictor = Predictor(input_size=input_size, hidden_size=hidden_size)\n",
    "predictor_optimizer = optim.SGD(predictor.parameters(), lr=0.001, momentum=0.9, weight_decay=1.5e-4)\n",
    "predictor_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 62/62 [00:05<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to checkpoints/pred_0.pth\n",
      "epoch: 00, loss: 2.680480480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 62/62 [00:05<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to checkpoints/pred_1.pth\n",
      "epoch: 01, loss: 2.597635031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 62/62 [00:05<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to checkpoints/pred_2.pth\n",
      "epoch: 02, loss: 2.489300013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 62/62 [00:05<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to checkpoints/pred_3.pth\n",
      "epoch: 03, loss: 2.335379601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 62/62 [00:05<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to checkpoints/pred_4.pth\n",
      "epoch: 04, loss: 2.074047804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 62/62 [00:05<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to checkpoints/pred_5.pth\n",
      "epoch: 05, loss: 1.538912535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 62/62 [00:05<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to checkpoints/pred_6.pth\n",
      "epoch: 06, loss: 0.891160190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 62/62 [00:05<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to checkpoints/pred_7.pth\n",
      "epoch: 07, loss: 0.549145520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 62/62 [00:05<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to checkpoints/pred_8.pth\n",
      "epoch: 08, loss: 0.369481415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 62/62 [00:05<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to checkpoints/pred_9.pth\n",
      "epoch: 09, loss: 0.262894452\n",
      "Training completed..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Predictor(\n",
       "  (lstm_cell): LSTMCell(2, 1024)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictor(predictor, encoder, dataloader, predictor_criterion, predictor_optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Inference if the Encoder and Decoder is part of the model.\n",
    "If the encoder is trained together with JEPA, we define the forward inference and training step.  \n",
    "The pending step is the defining the loss function and how to do backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JEPAModel(nn.Module):\n",
    "    def __init__(self, embed_size, input_channel_size):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleEncoder(embed_size, input_channel_size)\n",
    "        self.predictor = Predictor(2, 1024)\n",
    "        \n",
    "    def set_predictor(self, o, co):\n",
    "        so = self.encoder.forward(o)\n",
    "        self.predictor.set_hc(so, co)\n",
    "        return so\n",
    "    \n",
    "    def reset_predictor(self):\n",
    "        self.predictor.reset_hc()\n",
    "\n",
    "    def forward(self, action=None, state=None):\n",
    "        sy_hat, sy = None, None\n",
    "        if action is not None:\n",
    "            sy_hat = self.predictor(action)\n",
    "        if state is not None:\n",
    "            sy = self.encoder(state)\n",
    "\n",
    "        return sy_hat, sy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_inference(model, actions, states):\n",
    "    # shape of states = (b, L+1, c, h, w)\n",
    "    # shape of action = (b, L, 2)\n",
    "    B, L, D = state.shape[0], actions.shape[1], model.predictor.hidden_size\n",
    "\n",
    "    o = states[:, 0, :, :, :]\n",
    "    co = torch.zeros((B, D)).to(o.device)\n",
    "    model.set_predictor(o, co)\n",
    "\n",
    "    result = torch.empty((B, L, D))\n",
    "    for i in range(L):\n",
    "        sy_hat, _ = model(actions[:, i, :], states[:, i+1, :, :, :])\n",
    "        result[:, i, :] = sy_hat\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states shape: torch.Size([16, 17, 2, 65, 65])\n",
      "actions shape: torch.Size([16, 16, 2])\n",
      "torch.Size([16, 16, 1024])\n"
     ]
    }
   ],
   "source": [
    "model = JEPAModel(1024, 2)\n",
    "\n",
    "# first_datapoint = next(iter(dataloader))\n",
    "states, actions = first_datapoint\n",
    "model = model.to(device)\n",
    "states = states.to(device)\n",
    "actions = actions.to(device)\n",
    "\n",
    "print(f\"states shape: {states.shape}\")\n",
    "print(f\"actions shape: {actions.shape}\")\n",
    "\n",
    "result = forward_inference(model, actions, states)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are doing training of encoder and predictor together, then we need to handle all the different losses, defined in the figure\n",
    "\n",
    "![loss diagram](../../assets/loss_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_train_step(model, actions, states, optimizer, device, epochs=10):\n",
    "    B, L, D = state.shape[0], actions.shape[1], model.predictor.hidden_size\n",
    "\n",
    "    loss, loss1, loss2, loss3 = 0, 0, 0, 0\n",
    "\n",
    "    o = states[:, 0, :, :, :]\n",
    "    co = torch.zeros((B, D)).to(o.device)\n",
    "    so = model.set_predictor(o, co)\n",
    "    ## TODO: compute loss1 using `so`\n",
    "\n",
    "    for i in range(L):\n",
    "        sy_hat, sy = model(actions[:, i, :], states[:, i+1, :, :, :])\n",
    "        ## TODO: compute loss2 per iteration using `sy_hat`, `sy`\n",
    "        ## TODO: compute loss3 per iteration using `sy`\n",
    "        ## note: if we are using latent variables: we have to compute lossz\n",
    "        \n",
    "    loss = loss1 + loss2 + loss3\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_joint(model, dataloader, criterion_encoder, criterion_pred, optimizer, device, epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=\"Processing Batch\"):\n",
    "            state, action = batch\n",
    "            state, action = state.to(device), action.to(device)\n",
    "\n",
    "            B, L, D = state.shape[0], action.shape[1], model.predictor.hidden_size\n",
    "\n",
    "            loss, loss1, loss2, loss3 = 0, 0, 0, 0\n",
    "\n",
    "            o = states[:, 0, :, :, :]\n",
    "            c0 = torch.zeros((B, D)).to(device)\n",
    "            model.set_predictor(o, c0)\n",
    "            # compute loss1\n",
    "            loss1 = get_encoder_loss(model, o, transformation1, transformation2, criterion_encoder)\n",
    "\n",
    "            for i in range(L):\n",
    "                sy_hat, sy = model(action[:, i, :], state[:, i+1, :, :, :])\n",
    "                loss2 += criterion_pred(sy_hat, sy)\n",
    "                loss3 += get_encoder_loss(model, state[:, i, :, :, :], \n",
    "                                          transformation1, transformation2, criterion_encoder) \n",
    "            \n",
    "            # print(f\"loss1: {loss1.item()}, loss2: {loss2.item()}, loss3: {loss3.item()}\")\n",
    "            loss = loss1 + loss2 + loss3\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.detach()\n",
    "            if math.isnan(total_loss):\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch: {epoch}, total_loss: {total_loss}, the avg loss = {total_loss/len(dataloader)}\")\n",
    "        save_model(model, epoch, file_name=\"join_model\")\n",
    "\n",
    "    return model        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1.5e-4)\n",
    "criterion_predictor = nn.MSELoss()\n",
    "criterion_encoder = VICReg_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch: 100%|██████████| 62/62 [00:25<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, total_loss: 43199.1171875, the avg loss = 696.7599487304688\n",
      "Model saved to checkpoints/join_model_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch: 100%|██████████| 62/62 [00:26<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, total_loss: 42273.796875, the avg loss = 681.8353881835938\n",
      "Model saved to checkpoints/join_model_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch: 100%|██████████| 62/62 [00:26<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, total_loss: 42007.1875, the avg loss = 677.5352783203125\n",
      "Model saved to checkpoints/join_model_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JEPAModel(\n",
       "  (encoder): SimpleEncoder(\n",
       "    (conv1): Conv2d(2, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (pool1): MaxPool2d(kernel_size=(5, 5), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (pool2): MaxPool2d(kernel_size=(5, 5), stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "    (fc1): Linear(in_features=432, out_features=4096, bias=True)\n",
       "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  )\n",
       "  (predictor): Predictor(\n",
       "    (lstm_cell): LSTMCell(2, 1024)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the joint model\n",
    "train_joint(model, dataloader, criterion_encoder, criterion_predictor, joint_optimizer, device, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
